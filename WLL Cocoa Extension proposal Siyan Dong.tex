\documentclass[11pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath, amssymb}  % standard packages for math writing
\usepackage{listings}   % include the contents of code files
\usepackage{mathpazo} % a better font than the default
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{calligra}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsmath}
\setlength{\textwidth}{7in}
\setlength{\topmargin}{-.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{-.25in}
\setlength{\evensidemargin}{-.25in}
\setlength{\parindent}{0pt}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\FA}{\forall}
\newcommand{\IMP}{\implies}
\newcommand{\IFF}{\quad \textbf{iff} \quad}
\newcommand{\LG}{\langle}
\newcommand{\RG}{\rangle}
\newcommand{\LL}{L_2}
\newcommand{\WP}{\overset{wP1}{=}}
\newcommand{\WEQ}{\quad \text{w. eq. iff} \quad}
\newcommand{\IDP}{\perp\!\!\!\perp}
\newcommand{\Q}{\quad}
\newcommand{\DEF}{\overset{\text{def}}{=}}
\newcommand{\AND}{\Q\text{ and }\Q}

\newcommand{\THEO}[3]{
    \textbf{#1} \ 
    {#2}\
    \vspace*{-0.5\baselineskip} % Fixed vertical space
    \begin{align}
    #3
    \end{align}
}
\begin{document}
\textbf{3 questions I have}

1.
Looking at Equation (2.7), the WLL estimator $\hat{\beta}(x)$ acts as a convex combination of the pre-break and post-break estimators. My interpretation is that while the Local Linear estimator itself is first-order (fitting slopes), the mixing strategy via the weight matrix GAMMA is effectively 'zero-order'â€”it calculates a scalar weighted average of the two regime parameters.

Does this explain why the bias term involving the break size, $s_b \lambda(x)$, 
could not be diminished, as structural break setting, $\gamma > 0$? 
It seems the estimator forces a "compromise line' between the two regimes, 
and the only way the method solves this is by using the MFV to detect when this is just too costly (large $\lambda$), which will force $\gamma \to 0$?"
\\\\
2.
Regarding the high-dimensional limitation ($d<5$) you mentioned in the conclusion: 
is it accurate to interpret this 'curse of dimensionality' 
as the local window simply failing to capture the full geometry of $m_t(\cdot)$?
i.e using finite amount of parameters to capture a infinite diemensional non-parametric function.
Does this collapsion of infiite-NP into finite-P setting make both NP and P weaker than ML methods?

This is relevant because I plan to extend your framework for my Cocoa project 
using a Machine Learning model (like Random Forest) to handle high-dimensional indicators. 
I am wondering: if I replace the local linear estimator with a 'black box' ML model, 
would the asymptotic optimality of the Multifold Forward-Validation weights (Theorem 2.2) likely be preserved, 
or would that require a completely new consistency proof?
\\\\
3.
The paper's conclusion notes that constructing Prediction Intervals is a direction for future work. Given that my current Cocoa Project aims to quickly establish a benchmark comparison within a two-month deadline, I would like to ask for your advice: If I temporarily ignore variance breaks and interval forecasting to focus solely on comparing point forecast MSFE, would this simplified setup be sufficient for an 'entry-level project'? Or, in your opinion, is a mean forecast comparison not particularly meaningful for financial data without also modeling risk and volatility?


\pagebreak
\section{Data rationale}
\begin{table}[htbp]
\centering
\caption{Cocoa data: rationale and break-detection strategy}
\label{tab:cocoa-data}
\begin{tabular}{p{0.28\textwidth}p{0.64\textwidth}}
\textbf{Component} & \textbf{Description} \\

Data type and object &
Cocoa is a traded commodity; we work with log-returns $Y_t = \Delta \log P_t$ 
and covariates $X_t$ (e.g.\ weather variables). Following CGS, the series is viewed as a nonstationary process generated 
by a time-varying conditional mean function $m_t(x)$ . \\[0.6em]

Structural breaks &
In the CGS framework, nonstationarity arises through 
time-variation in $m_a(\cdot) \neq m_b(\cdot)$. 
Cocoa prices are plausibly nonstationary and subject to structural breaks driven by climate 
and supply-chain shocks (e.g.\ El nino supply disruptions), 
which induce discrete changes visually in the conditional mean/variance. \\[0.6em]

Economic interpretation of breaks &
El nino is widely believed to be dominant the break in the conditional mean of log-returns. 
This aligns with the CGS notion of a structural break as a change in the regression
 function, and yields an economically interpretability before and after break \\[0.6em]

Break number and size &
There are several identifiable breaks in the Cocoa log-return series, 
however, dominant by a single large break after 2024 El nino event which we argued earlier.
This break has significant volatility and mean shifts. For sake of comparison, 
we will only validate the CGS framework until end of 2024. \\[0.6em]

Break detection strategy (Mohr and Selk, 2020) &
To determine the break date $\tau$ in a data-driven way, 
Employ as Cai et al suggested, Mohr and Selk (2020). This is plausible under our assumption of Log return being strong mixing. 
Matches the break-detection step recommended in the CGS framework. \\[0.6em]

Within-regime approximation &
Conditional on the estimated break date $\tau$, we treat the process as piecewise stationary and 
strongly mixing within each regime(pre and post). 
This allows us to apply the CGS weighted local linear procedures 
and asymptotic theory locally within each regime. \\

\end{tabular}
\end{table}
In summary, 
the Cocoa series fits naturally into the CGS framework of nonparametric time-varying regression 
with a single structural break. A well designed experimental setup with baseline methods will allow us 
to validate the CGS method's effectiveness in this real-world scenario.

\section{Experiment setup and models}

\begin{table}[htbp]
    \centering
    \caption{Proposed Model Architecture for Cocoa Price Forecasting}
    \label{tab:models}
    \renewcommand{\arraystretch}{1.5} % Increases row height for readability
    \begin{tabular}{@{} l l l p{6.5cm} @{}}
        \toprule
        \textbf{Group} & \textbf{Model Name} & \textbf{Data Usage} & \textbf{Mechanism \& Rationale} \\
        \midrule
        
        % GROUP 1: NP Benchmarks
        \multirow{9}{*}{\textbf{1. NP Benchmarks}} 
        & \textbf{Post-Break LL} & Post-break & \textbf{Low bias, high variance baseline} \\
        & (PBLL) &  & Uses Local Linear estimation only on new regime data. Low bias, potentially high variance. \\
        \cmidrule(lr){2-4}
        
        & \textbf{Pre-Break LL} & Pre-break & \textbf{High Bias, low var baseline} \\
        & (PrBLL) &  & Assumes the old regime persists. Used to quantify the magnitude of the structural break ($\lambda(x)$). \\
        \cmidrule(lr){2-4}
        
        & \textbf{Weighted LL} & Combined & \textbf{Cai et al., 2025} \\
        & (WLL) &  & Convex combination of Pre/Post estimators. Uses \textbf{MFV} to select $\gamma$, balancing bias from old data against variance of new data. \\
        \midrule
        
        % GROUP 2: ML Competitors
        \multirow{6}{*}{\textbf{2. ML Competitors}} 
        & \textbf{XGBoost} & Full Sample & \textbf{Gradient Boosting Standard.} \\
        & & (with Time Index) & Tests if a "black box" learner can implicitly adapt to the structural break without explicit weighting. \\
        \cmidrule(lr){2-4}
        
        & \textbf{Random Forest} & Full Sample & \textbf{Bagging Standard.} \\
        & & (with Time Index) & Used to evaluate if lag is more pronounced in bagging methods compared to WLL. \\
        \midrule
        
        % GROUP 3: ML Extension
        \multirow{5}{*}{\textbf{3. ML Extension}} 
        & \textbf{Weighted ML} & Combined & \textbf{The Methodological Mirror.} \\
        & \textbf{Ensemble} & & Replaces Local Linear regressors with ML models. Explicitly applies the Cai et al. (2025) weighting strategy to ML outputs via MFV. \\
        
        \bottomrule
    \end{tabular}
    Combined means using Cai et al's linear combination between regimes idea.$\gamma \text{Pre} + (1-\gamma) \text{Post}$

\end{table}
\pagebreak
\subsection{Data preprocess}
Drop daily highest/lowest temperature records, there are no further missing values in Precipitation 
and Average temperature. Then since more than 1 station might record data for the same day,
we will use anomoly valuse averaging from past data to aggregate them into a single daily record without peaking ahead.

After that we create log price and log return from daily price data. 
Then we merge weather data with log-return data by date and drop the missing values. 

\subsection{Model strucuture}
We strictly follow the CGS framework to set up our models. 
But aware by the point out of Cai et al, that the recommended dimension will be less than 5. So this strictly limits the number of past lags we can use as features.
We will also strictly follow the CGS cross-validation procedure to select hyperparameters.
i.e MFV set to $Q =4, etc$. 

\section{Evaluation metrics}
We use a primary MSFE as Cai et al 2025 did, focusing on OOS performance in the post-break regime:
Secondary metric will be using MDM to compare the performance between models pairwise.


\subsection{Hypothesis}
While ML models without break weighting strategy may achieve low overall MSFE, however,
WLL estimator will demonstrate fast responding adatpion in the immediate post-break period by lower cumulative error. 
This would suggest the explicit weighting strategy is effective in handling structural breaks in nonparametric time series forecasting.

i.e penalize $\lambda(x)$ quick enough to diminish the bias term if necessary.











\end{document}